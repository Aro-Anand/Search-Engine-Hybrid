# Product Requirements Document (PRD)
# Semantic Search Engine for Small-Scale Listings

**Version:** 1.0  
**Date:** February 8, 2026  
**Author:** Development Team  
**Status:** Ready for Implementation

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Project Overview](#project-overview)
3. [System Architecture](#system-architecture)
4. [Technical Specifications](#technical-specifications)
5. [Implementation Roadmap](#implementation-roadmap)
6. [Deployment Guide (AWS EC2)](#deployment-guide)
7. [Code Quality & Standards](#code-quality-standards)
8. [Testing Strategy](#testing-strategy)
9. [Performance Requirements](#performance-requirements)
10. [Security Considerations](#security-considerations)
11. [Maintenance & Monitoring](#maintenance-monitoring)
12. [Future Enhancements](#future-enhancements)

---

## 1. Executive Summary

This document outlines the requirements and implementation plan for a production-ready semantic search engine optimized for small-scale datasets (200-10,000 listings). The system combines keyword-based filtering with semantic similarity using transformer models to provide Google-like search experiences with autocomplete, recent searches, and intelligent ranking.

**Key Objectives:**
- Fast, accurate search across 200+ product/service listings
- Real-time autocomplete suggestions
- User-specific recent search history
- Sub-100ms response time for typical queries
- Production-ready code with comprehensive documentation
- Easy deployment on AWS EC2

---

## 2. Project Overview

### 2.1 Problem Statement

Users need to quickly find relevant listings from a website's inventory using natural language queries, similar to how Google Search operates. Traditional keyword matching fails to capture semantic meaning ("affordable laptop" should match "budget notebook computer").

### 2.2 Solution

A hybrid search engine that:
1. Pre-filters using fast keyword matching
2. Re-ranks results using semantic similarity (sentence transformers)
3. Provides autocomplete suggestions as users type
4. Tracks recent searches per user session

### 2.3 Target Audience

- **Primary Users:** Website visitors searching for listings
- **Secondary Users:** Frontend developers integrating the API
- **Administrators:** DevOps engineers deploying and maintaining the service

### 2.4 Success Metrics

- Search latency < 100ms for 95th percentile
- Autocomplete suggestions < 50ms
- Search relevance: >80% user satisfaction (via click-through rate)
- System uptime: 99.5%

---

## 3. System Architecture

### 3.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚
â”‚   (React/Vue)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/REST
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Server (Python)         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Search Engine Core              â”‚  â”‚
â”‚  â”‚  - Keyword Filter                 â”‚  â”‚
â”‚  â”‚  - Semantic Re-ranker             â”‚  â”‚
â”‚  â”‚  - Autocomplete Engine            â”‚  â”‚
â”‚  â”‚  - Recent Searches Manager        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Listings Data  â”‚
â”‚  (JSON/SQLite)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Component Breakdown

| Component | Technology | Purpose |
|-----------|-----------|---------|
| API Server | FastAPI + Uvicorn | REST endpoints for search, autocomplete, recent searches |
| Search Engine | Python + sentence-transformers | Hybrid keyword + semantic search |
| Autocomplete | Trie data structure | Fast prefix matching |
| Recent Searches | Redis (optional) or in-memory dict | User session management |
| Data Storage | JSON file or SQLite | Listing metadata |
| Embeddings | NumPy arrays | Pre-computed semantic vectors |

### 3.3 Data Flow

1. **Indexing (One-time startup):**
   ```
   Listings JSON â†’ Extract text â†’ Generate embeddings â†’ Store in memory
   ```

2. **Search Request:**
   ```
   User query â†’ Keyword filter â†’ Semantic ranking â†’ Top K results â†’ JSON response
   ```

3. **Autocomplete:**
   ```
   User input â†’ Trie lookup â†’ Popular terms â†’ JSON response
   ```

---

## 4. Technical Specifications

### 4.1 Technology Stack

| Layer | Technology | Version | Justification |
|-------|-----------|---------|---------------|
| Runtime | Python | 3.9+ | ML ecosystem, async support |
| Web Framework | FastAPI | 0.109+ | Fast, async, auto-docs |
| Server | Uvicorn | 0.27+ | ASGI server for production |
| ML Model | sentence-transformers | 2.3+ | State-of-art embeddings |
| Model | all-MiniLM-L6-v2 | - | Fast, 384-dim, good accuracy |
| Numerical | NumPy | 1.24+ | Vector operations |
| Search | scikit-learn | 1.4+ | Cosine similarity |
| Storage (optional) | SQLite | 3.35+ | Embedded database |
| Cache (optional) | Redis | 7.0+ | Session management |

### 4.2 API Endpoints

#### 4.2.1 Search Endpoint

```http
GET /api/v1/search?q={query}&limit={limit}&offset={offset}
```

**Parameters:**
- `q` (required): Search query string
- `limit` (optional): Max results, default 10
- `offset` (optional): Pagination offset, default 0

**Response:**
```json
{
  "query": "affordable laptop",
  "total_results": 42,
  "results": [
    {
      "id": "listing_123",
      "title": "Budget Notebook Computer",
      "description": "...",
      "score": 0.87,
      "matched_keywords": ["laptop", "affordable"]
    }
  ],
  "processing_time_ms": 45
}
```

#### 4.2.2 Autocomplete Endpoint

```http
GET /api/v1/autocomplete?q={partial_query}&limit={limit}
```

**Parameters:**
- `q` (required): Partial query (1-50 chars)
- `limit` (optional): Max suggestions, default 5

**Response:**
```json
{
  "suggestions": [
    "laptop",
    "laptop bag",
    "laptop stand"
  ]
}
```

#### 4.2.3 Recent Searches

```http
GET /api/v1/recent/{user_id}
POST /api/v1/recent
DELETE /api/v1/recent/{user_id}
```

**POST Body:**
```json
{
  "user_id": "user_abc123",
  "query": "laptop"
}
```

**GET Response:**
```json
{
  "user_id": "user_abc123",
  "recent_searches": [
    "laptop",
    "wireless mouse",
    "keyboard"
  ]
}
```

### 4.3 Database Schema (SQLite - Optional)

```sql
-- Listings table
CREATE TABLE listings (
    id TEXT PRIMARY KEY,
    title TEXT NOT NULL,
    description TEXT,
    category TEXT,
    price REAL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    embedding BLOB  -- Serialized NumPy array
);

-- Search terms for autocomplete
CREATE TABLE search_terms (
    term TEXT PRIMARY KEY,
    frequency INTEGER DEFAULT 1,
    last_searched TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- User recent searches
CREATE TABLE recent_searches (
    user_id TEXT NOT NULL,
    query TEXT NOT NULL,
    searched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (user_id, query)
);
CREATE INDEX idx_user_recent ON recent_searches(user_id, searched_at DESC);
```

### 4.4 File-Based Storage (Alternative)

For 200 listings, simple JSON files are sufficient:

```
data/
â”œâ”€â”€ listings.json          # All listing data
â”œâ”€â”€ embeddings.npy         # Pre-computed embeddings
â””â”€â”€ search_terms.json      # Popular search terms
```

---

## 5. Implementation Roadmap

### Phase 1: Core Search Engine (Week 1)

**Goal:** Basic search functionality with keyword + semantic ranking

**Tasks:**
1. âœ… Set up project structure
2. âœ… Implement `HybridSearchEngine` class
3. âœ… Create keyword filter
4. âœ… Integrate sentence-transformers
5. âœ… Write unit tests
6. âœ… Create sample dataset

**Deliverables:**
- Working search engine (command-line interface)
- 80%+ test coverage
- Performance benchmarks

### Phase 2: API Development (Week 2)

**Goal:** REST API with all endpoints

**Tasks:**
1. âœ… Set up FastAPI project
2. âœ… Implement `/search` endpoint
3. âœ… Implement `/autocomplete` endpoint
4. âœ… Implement `/recent` endpoints
5. âœ… Add request validation
6. âœ… Set up CORS
7. âœ… Write API documentation

**Deliverables:**
- Fully functional REST API
- Swagger/OpenAPI docs
- Postman collection

### Phase 3: Optimization & Production Readiness (Week 3)

**Goal:** Production-quality code with monitoring

**Tasks:**
1. âœ… Add caching layer
2. âœ… Implement rate limiting
3. âœ… Add logging (structured JSON logs)
4. âœ… Set up health check endpoint
5. âœ… Optimize startup time
6. âœ… Add metrics (Prometheus format)
7. âœ… Write deployment scripts

**Deliverables:**
- Production-ready codebase
- Deployment documentation
- Monitoring dashboard

### Phase 4: Deployment & Documentation (Week 4)

**Goal:** Live deployment on AWS EC2

**Tasks:**
1. âœ… Create EC2 instance
2. âœ… Configure security groups
3. âœ… Set up systemd service
4. âœ… Configure nginx reverse proxy
5. âœ… Set up SSL (Let's Encrypt)
6. âœ… Write user documentation
7. âœ… Create video walkthrough

**Deliverables:**
- Live production endpoint
- Complete documentation
- Handoff to frontend team

---

## 6. Deployment Guide (AWS EC2)

### 6.1 Server Requirements

**Recommended EC2 Instance:**
- **Type:** `t3.medium` (2 vCPU, 4 GB RAM)
- **Storage:** 20 GB SSD (General Purpose)
- **OS:** Ubuntu 22.04 LTS
- **Estimated Cost:** ~$30/month

**Memory Breakdown:**
```
OS + System:        ~1.0 GB
Python Runtime:     ~0.5 GB
Model (MiniLM):     ~0.3 GB
Embeddings (200):   ~0.1 GB
API Server:         ~0.5 GB
Headroom:           ~1.6 GB
----------------------------
Total:              ~4.0 GB
```

**For 1,000 listings:** Same instance works fine  
**For 10,000+ listings:** Upgrade to `t3.large` (8 GB RAM)

### 6.2 Step-by-Step Deployment

#### Step 1: Launch EC2 Instance

```bash
# Via AWS Console:
# 1. Go to EC2 Dashboard
# 2. Launch Instance â†’ Ubuntu 22.04 LTS
# 3. Instance type: t3.medium
# 4. Configure security group (see below)
# 5. Create/select key pair
# 6. Launch instance
```

**Security Group Rules:**
| Type | Protocol | Port | Source | Purpose |
|------|----------|------|--------|---------|
| SSH | TCP | 22 | Your IP | Admin access |
| HTTP | TCP | 80 | 0.0.0.0/0 | Web traffic |
| HTTPS | TCP | 443 | 0.0.0.0/0 | Secure web traffic |
| Custom | TCP | 8000 | 0.0.0.0/0 | API (temp, remove after nginx) |

#### Step 2: Connect and Update

```bash
# Connect to instance
ssh -i your-key.pem ubuntu@<instance-ip>

# Update system
sudo apt update && sudo apt upgrade -y

# Install dependencies
sudo apt install -y python3.9 python3-pip nginx git
```

#### Step 3: Clone and Setup Project

```bash
# Clone repository
cd /home/ubuntu
git clone https://github.com/yourusername/semantic-search-engine.git
cd semantic-search-engine

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

#### Step 4: Configure Environment

```bash
# Create .env file
cat > .env << EOF
ENVIRONMENT=production
API_HOST=0.0.0.0
API_PORT=8000
WORKERS=2
LOG_LEVEL=info
ENABLE_CORS=true
ALLOWED_ORIGINS=https://yourwebsite.com
EOF
```

#### Step 5: Test Application

```bash
# Run locally first
python -m app.main

# Test in another terminal
curl http://localhost:8000/health
curl "http://localhost:8000/api/v1/search?q=laptop&limit=5"
```

#### Step 6: Set up Systemd Service

```bash
# Create service file
sudo nano /etc/systemd/system/semantic-search.service
```

```ini
[Unit]
Description=Semantic Search API
After=network.target

[Service]
Type=simple
User=ubuntu
WorkingDirectory=/home/ubuntu/semantic-search-engine
Environment="PATH=/home/ubuntu/semantic-search-engine/venv/bin"
ExecStart=/home/ubuntu/semantic-search-engine/venv/bin/uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 2
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start service
sudo systemctl daemon-reload
sudo systemctl enable semantic-search
sudo systemctl start semantic-search
sudo systemctl status semantic-search

# View logs
sudo journalctl -u semantic-search -f
```

#### Step 7: Configure Nginx Reverse Proxy

```bash
# Create nginx config
sudo nano /etc/nginx/sites-available/semantic-search
```

```nginx
server {
    listen 80;
    server_name your-domain.com;

    location / {
        proxy_pass http://127.0.0.1:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

```bash
# Enable site
sudo ln -s /etc/nginx/sites-available/semantic-search /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx
```

#### Step 8: Set up SSL (Optional but Recommended)

```bash
# Install certbot
sudo apt install -y certbot python3-certbot-nginx

# Get SSL certificate
sudo certbot --nginx -d your-domain.com

# Auto-renewal is configured automatically
sudo certbot renew --dry-run
```

#### Step 9: Verify Deployment

```bash
# Test public endpoint
curl https://your-domain.com/health
curl "https://your-domain.com/api/v1/search?q=test"
```

### 6.3 Monitoring & Maintenance

```bash
# Check service status
sudo systemctl status semantic-search

# View logs (last 100 lines)
sudo journalctl -u semantic-search -n 100

# Restart service
sudo systemctl restart semantic-search

# Update application
cd /home/ubuntu/semantic-search-engine
git pull
source venv/bin/activate
pip install -r requirements.txt
sudo systemctl restart semantic-search
```

---

## 7. Code Quality & Standards

### 7.1 Project Structure

```
semantic-search-engine/
â”œâ”€â”€ README.md                    # Project overview, quick start
â”œâ”€â”€ PRD.md                       # This document
â”œâ”€â”€ LICENSE                      # MIT License
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ requirements-dev.txt         # Development dependencies
â”œâ”€â”€ .env.example                 # Environment variables template
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ setup.py                     # Package installation
â”‚
â”œâ”€â”€ app/                         # Main application package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                # Configuration management
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                    # Core business logic
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ search_engine.py     # HybridSearchEngine class
â”‚   â”‚   â”œâ”€â”€ autocomplete.py      # AutocompleteEngine class
â”‚   â”‚   â”œâ”€â”€ recent_searches.py   # RecentSearchManager class
â”‚   â”‚   â””â”€â”€ embeddings.py        # Embedding utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                     # API routes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ search.py        # /search endpoint
â”‚   â”‚   â”‚   â”œâ”€â”€ autocomplete.py  # /autocomplete endpoint
â”‚   â”‚   â”‚   â””â”€â”€ recent.py        # /recent endpoints
â”‚   â”‚   â””â”€â”€ health.py            # Health check
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                  # Pydantic models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ request.py           # Request schemas
â”‚   â”‚   â””â”€â”€ response.py          # Response schemas
â”‚   â”‚
â”‚   â””â”€â”€ utils/                   # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py            # Logging setup
â”‚       â”œâ”€â”€ metrics.py           # Performance metrics
â”‚       â””â”€â”€ validators.py        # Input validation
â”‚
â”œâ”€â”€ data/                        # Data files
â”‚   â”œâ”€â”€ listings.json            # Sample listings
â”‚   â”œâ”€â”€ embeddings.npy           # Pre-computed embeddings
â”‚   â””â”€â”€ search_terms.json        # Popular terms
â”‚
â”œâ”€â”€ tests/                       # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py              # Pytest fixtures
â”‚   â”œâ”€â”€ test_search_engine.py
â”‚   â”œâ”€â”€ test_autocomplete.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ scripts/                     # Utility scripts
â”‚   â”œâ”€â”€ generate_embeddings.py   # Pre-compute embeddings
â”‚   â”œâ”€â”€ import_listings.py       # Import data from CSV/JSON
â”‚   â””â”€â”€ benchmark.py             # Performance testing
â”‚
â”œâ”€â”€ docs/                        # Documentation
â”‚   â”œâ”€â”€ API.md                   # API documentation
â”‚   â”œâ”€â”€ DEPLOYMENT.md            # Deployment guide
â”‚   â”œâ”€â”€ CONTRIBUTING.md          # Contribution guidelines
â”‚   â””â”€â”€ ARCHITECTURE.md          # System architecture
â”‚
â””â”€â”€ deployment/                  # Deployment files
    â”œâ”€â”€ systemd/
    â”‚   â””â”€â”€ semantic-search.service
    â”œâ”€â”€ nginx/
    â”‚   â””â”€â”€ semantic-search.conf
    â””â”€â”€ docker/
        â”œâ”€â”€ Dockerfile
        â””â”€â”€ docker-compose.yml
```

### 7.2 Code Documentation Standards

#### 7.2.1 README.md Template

```markdown
# Semantic Search Engine

Fast, intelligent search for small-scale datasets using hybrid keyword + semantic matching.

## Features

- âš¡ Sub-100ms search latency
- ğŸ§  Semantic understanding via sentence transformers
- ğŸ” Google-like autocomplete
- ğŸ“± RESTful API
- ğŸš€ Production-ready

## Quick Start

### Installation

\`\`\`bash
# Clone repository
git clone https://github.com/yourusername/semantic-search-engine.git
cd semantic-search-engine

# Install dependencies
pip install -r requirements.txt

# Run server
python -m app.main
\`\`\`

### Usage

\`\`\`bash
# Search
curl "http://localhost:8000/api/v1/search?q=laptop&limit=5"

# Autocomplete
curl "http://localhost:8000/api/v1/autocomplete?q=lap"
\`\`\`

## Documentation

- [API Documentation](docs/API.md)
- [Deployment Guide](docs/DEPLOYMENT.md)
- [Architecture Overview](docs/ARCHITECTURE.md)

## Tech Stack

- Python 3.9+
- FastAPI
- sentence-transformers (all-MiniLM-L6-v2)
- NumPy, scikit-learn

## License

MIT
```

#### 7.2.2 Module-Level Docstrings

```python
"""
Search Engine Core Module

This module implements the hybrid search engine that combines keyword-based
filtering with semantic similarity ranking.

Classes:
    HybridSearchEngine: Main search engine orchestrator
    
Functions:
    preprocess_query: Clean and normalize user queries
    
Example:
    >>> engine = HybridSearchEngine()
    >>> engine.index_listings(listings)
    >>> results = engine.search("affordable laptop", top_k=10)
"""
```

#### 7.2.3 Class Documentation

```python
class HybridSearchEngine:
    """
    Hybrid search engine combining keyword matching and semantic similarity.
    
    This class implements a two-stage search pipeline:
    1. Fast keyword filtering to reduce candidate set
    2. Semantic re-ranking using sentence embeddings
    
    Attributes:
        model (SentenceTransformer): Embedding model for semantic search
        listings (List[Dict]): Indexed listing documents
        embeddings (np.ndarray): Pre-computed document embeddings
        autocomplete (AutocompleteEngine): Autocomplete suggestion engine
    
    Example:
        >>> engine = HybridSearchEngine()
        >>> listings = [{"id": "1", "title": "Laptop", "description": "..."}]
        >>> engine.index_listings(listings)
        >>> results = engine.search("laptop", top_k=5)
        >>> print(len(results))  # 5
    """
    
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        Search listings using hybrid keyword + semantic approach.
        
        Args:
            query: User search query (1-200 characters)
            top_k: Maximum number of results to return (default: 10)
        
        Returns:
            List of matching listings sorted by relevance score. Each listing
            contains original fields plus 'score' (0.0-1.0) and 'match_type'.
        
        Raises:
            ValueError: If query is empty or too long
            RuntimeError: If engine not initialized with index_listings()
        
        Example:
            >>> results = engine.search("budget laptop", top_k=3)
            >>> results[0]['score']
            0.87
            >>> results[0]['match_type']
            'semantic'
        """
        pass
```

#### 7.2.4 Inline Comments

```python
# Good: Explains WHY, not WHAT
# Use keyword filter first to reduce search space from O(n) to O(k)
# where k << n, improving semantic ranking performance
keyword_matches = self._keyword_search(query)

# Bad: Obvious comment
# Loop through matches
for match in keyword_matches:
    pass
```

### 7.3 Code Style Guide

**Follow PEP 8 with these additions:**

```python
# Type hints for all function signatures
def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
    pass

# Use f-strings for formatting
logger.info(f"Search query: {query}, found {len(results)} results")

# Descriptive variable names
embedding_matrix = self.model.encode(texts)  # Good
emb = self.model.encode(texts)               # Bad

# Constants in UPPER_CASE
MAX_QUERY_LENGTH = 200
DEFAULT_TOP_K = 10

# Class names in PascalCase
class HybridSearchEngine:
    pass

# Functions in snake_case
def preprocess_query(query: str) -> str:
    pass
```

### 7.4 Error Handling

```python
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

class SearchError(Exception):
    """Base exception for search errors"""
    pass

class EngineNotInitializedError(SearchError):
    """Raised when search is called before indexing"""
    pass

def search(self, query: str, top_k: int = 10) -> List[Dict]:
    """Search with comprehensive error handling"""
    
    # Input validation
    if not query or not query.strip():
        raise ValueError("Query cannot be empty")
    
    if len(query) > MAX_QUERY_LENGTH:
        raise ValueError(f"Query too long (max {MAX_QUERY_LENGTH} chars)")
    
    if top_k < 1 or top_k > 100:
        raise ValueError("top_k must be between 1 and 100")
    
    # State validation
    if self.embeddings is None:
        raise EngineNotInitializedError(
            "Search engine not initialized. Call index_listings() first."
        )
    
    try:
        # Perform search
        results = self._execute_search(query, top_k)
        logger.info(f"Search successful: query='{query}', results={len(results)}")
        return results
        
    except Exception as e:
        logger.error(f"Search failed: {e}", exc_info=True)
        raise SearchError(f"Search failed: {str(e)}") from e
```

### 7.5 Testing Requirements

**Minimum 80% code coverage**

```python
# tests/test_search_engine.py
import pytest
from app.core.search_engine import HybridSearchEngine

@pytest.fixture
def sample_listings():
    """Fixture providing sample listing data"""
    return [
        {"id": "1", "title": "Gaming Laptop", "description": "High-performance laptop"},
        {"id": "2", "title": "Budget Notebook", "description": "Affordable computer"},
        {"id": "3", "title": "Office Desktop", "description": "Business workstation"},
    ]

@pytest.fixture
def search_engine(sample_listings):
    """Fixture providing initialized search engine"""
    engine = HybridSearchEngine()
    engine.index_listings(sample_listings)
    return engine

class TestHybridSearchEngine:
    """Test suite for HybridSearchEngine"""
    
    def test_search_returns_results(self, search_engine):
        """Test that search returns non-empty results for valid query"""
        results = search_engine.search("laptop", top_k=5)
        assert len(results) > 0
        assert all('score' in r for r in results)
    
    def test_search_respects_top_k(self, search_engine):
        """Test that search returns at most top_k results"""
        results = search_engine.search("computer", top_k=2)
        assert len(results) <= 2
    
    def test_search_empty_query_raises_error(self, search_engine):
        """Test that empty query raises ValueError"""
        with pytest.raises(ValueError, match="empty"):
            search_engine.search("")
    
    def test_semantic_similarity(self, search_engine):
        """Test that semantically similar terms match"""
        # "notebook" should match "laptop" even without exact keyword match
        results = search_engine.search("notebook", top_k=5)
        titles = [r['title'].lower() for r in results]
        assert any('laptop' in title or 'notebook' in title for title in titles)
```

---

## 8. Testing Strategy

### 8.1 Test Levels

| Test Type | Coverage | Tools | Frequency |
|-----------|----------|-------|-----------|
| Unit Tests | Core logic, utilities | pytest | Every commit |
| Integration Tests | API endpoints | pytest + httpx | Every PR |
| Performance Tests | Latency, throughput | locust | Weekly |
| Load Tests | Concurrent users | locust | Pre-deployment |

### 8.2 Test Commands

```bash
# Run all tests
pytest

# Run with coverage report
pytest --cov=app --cov-report=html --cov-report=term

# Run specific test file
pytest tests/test_search_engine.py -v

# Run tests matching pattern
pytest -k "test_search" -v

# Performance benchmark
python scripts/benchmark.py
```

---

## 9. Performance Requirements

### 9.1 Latency Targets

| Operation | Target (p50) | Target (p95) | Target (p99) |
|-----------|--------------|--------------|--------------|
| Search | 50ms | 100ms | 200ms |
| Autocomplete | 20ms | 50ms | 100ms |
| Recent Searches (GET) | 10ms | 20ms | 50ms |
| Health Check | 5ms | 10ms | 20ms |

### 9.2 Throughput Targets

- **Search:** 100 requests/second (single t3.medium instance)
- **Autocomplete:** 200 requests/second
- **Concurrent Users:** 50+ simultaneous searches

### 9.3 Resource Limits

- **Memory:** < 3.5 GB under normal load
- **CPU:** < 70% average utilization
- **Disk I/O:** < 10 MB/s

---

## 10. Security Considerations

### 10.1 Input Validation

```python
# Prevent injection attacks
MAX_QUERY_LENGTH = 200
ALLOWED_QUERY_CHARS = r'^[a-zA-Z0-9\s\-_.,!?]+$'

def validate_query(query: str) -> str:
    """Validate and sanitize user query"""
    if len(query) > MAX_QUERY_LENGTH:
        raise ValueError("Query too long")
    
    if not re.match(ALLOWED_QUERY_CHARS, query):
        raise ValueError("Query contains invalid characters")
    
    return query.strip()
```

### 10.2 Rate Limiting

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.get("/api/v1/search")
@limiter.limit("100/minute")
async def search(request: Request, q: str):
    # Search logic
    pass
```

### 10.3 CORS Configuration

```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["https://yourwebsite.com"],  # Specific domains only
    allow_credentials=True,
    allow_methods=["GET", "POST"],
    allow_headers=["*"],
)
```

---

## 11. Maintenance & Monitoring

### 11.1 Logging

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    """Output logs in JSON format for structured logging"""
    
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
        }
        
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        
        return json.dumps(log_data)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("/var/log/semantic-search/app.log")
    ]
)

for handler in logging.root.handlers:
    handler.setFormatter(JSONFormatter())
```

### 11.2 Health Check Endpoint

```python
@app.get("/health")
async def health_check():
    """
    Comprehensive health check for monitoring systems
    
    Returns service status, uptime, and key metrics
    """
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "uptime_seconds": time.time() - app.state.start_time,
        "indexed_listings": len(app.state.search_engine.listings),
        "model_loaded": app.state.search_engine.model is not None,
        "version": "1.0.0"
    }
```

### 11.3 Metrics Endpoint (Prometheus)

```python
from prometheus_client import Counter, Histogram, generate_latest

# Define metrics
search_requests = Counter('search_requests_total', 'Total search requests')
search_latency = Histogram('search_latency_seconds', 'Search latency')

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(content=generate_latest(), media_type="text/plain")
```

---

## 12. Future Enhancements

### 12.1 Short-Term (1-3 months)

1. **Query Expansion**
   - Add synonym support (e.g., "laptop" â†’ "notebook", "computer")
   - Spell correction for typos

2. **Personalization**
   - User-specific ranking based on past clicks
   - A/B testing framework

3. **Analytics Dashboard**
   - Popular search terms
   - Zero-result queries
   - Click-through rates

### 12.2 Medium-Term (3-6 months)

1. **Advanced Filtering**
   - Price range filters
   - Category facets
   - Date range filtering

2. **Multi-language Support**
   - Multilingual embedding models
   - Language detection

3. **Caching Layer**
   - Redis for popular queries
   - Edge caching (CloudFront)

### 12.3 Long-Term (6+ months)

1. **Machine Learning Improvements**
   - Fine-tune embedding model on domain data
   - Learning-to-rank with user feedback

2. **Scalability**
   - Vector database (Pinecone/Weaviate) for 100k+ listings
   - Horizontal scaling with load balancer

3. **Advanced Features**
   - Image search
   - Voice search support
   - Recommendation engine

---

## Appendix A: Glossary

| Term | Definition |
|------|------------|
| Embedding | Numerical vector representation of text |
| Semantic Search | Search based on meaning, not just keywords |
| Cosine Similarity | Measure of similarity between two vectors |
| Trie | Tree data structure for prefix matching |
| FastAPI | Modern Python web framework |
| sentence-transformers | Library for creating text embeddings |
| Uvicorn | ASGI web server |
| DXA | Drawing Units (1/1440 inch in Word docs) |

---

## Appendix B: References

1. [FastAPI Documentation](https://fastapi.tiangolo.com/)
2. [sentence-transformers](https://www.sbert.net/)
3. [AWS EC2 Pricing](https://aws.amazon.com/ec2/pricing/)
4. [Python Best Practices](https://docs.python-guide.org/)
5. [Semantic Search Explained](https://www.pinecone.io/learn/semantic-search/)

---

## Appendix C: Contact & Support

**Project Maintainer:** [Your Name]  
**Email:** [your.email@example.com]  
**GitHub Issues:** [https://github.com/yourusername/semantic-search-engine/issues]  
**Documentation:** [https://docs.yourproject.com]

---

**Document Version History:**

| Version | Date | Author | Changes |
|---------|------|--------|---------|
| 1.0 | 2026-02-08 | Development Team | Initial PRD |

